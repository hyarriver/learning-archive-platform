# 学习资料聚合平台 - 项目完成说明

## 项目完成情况

### ✅ 已完成的核心模块

1. **工具模块** (`backend/app/utils/`)
   - 日志配置 (`logger.py`)
   - 认证工具 (`auth.py`) - JWT令牌生成/验证、密码加密
   - 通用工具 (`helpers.py`) - 文件名清理、哈希计算等

2. **爬虫模块** (`backend/app/crawler/`)
   - 基础爬虫类 (`base.py`) - 统一的爬虫接口、重试机制
   - 网页爬虫 (`webpage.py`) - 支持静态网页抓取
   - 视频爬虫 (`video.py`) - 使用 yt-dlp 提取视频字幕
   - 内容解析器 (`parser.py`) - HTML内容提取和清理

3. **转换模块** (`backend/app/converter/`)
   - Markdown转换 (`to_markdown.py`) - HTML转Markdown（支持pandoc和html2text）
   - 目录生成 (`toc_generator.py`) - 自动生成Markdown目录
   - 标签提取 (`tag_extractor.py`) - 基于关键词提取标签
   - 摘要生成 (`summary_generator.py`) - 自动生成内容摘要

4. **存储模块** (`backend/app/storage/`)
   - 文件管理 (`file_manager.py`) - 文件保存、读取、删除
   - 版本管理 (`version_manager.py`) - 文件版本控制和历史记录

5. **任务调度模块** (`backend/app/scheduler/`)
   - 定时任务 (`tasks.py`) - APScheduler集成，支持每日定时采集

6. **API路由模块** (`backend/app/api/`)
   - 认证API (`auth.py`) - 用户登录、获取当前用户信息
   - 文件API (`files.py`) - 文件列表、详情、下载、版本查询
   - 采集管理API (`collection.py`) - 采集源管理、手动触发采集
   - 用户管理API (`users.py`) - 用户创建、列表

7. **FastAPI主应用** (`backend/app/main.py`)
   - 应用生命周期管理
   - 路由注册和中间件配置
   - CORS支持

8. **前端界面** (`frontend/`)
   - 登录页面
   - 文件列表页面（支持搜索、分页）
   - 文件详情页面（Markdown预览）
   - 采集管理页面

## 技术架构

### 后端
- **框架**: FastAPI
- **数据库**: SQLite（使用SQLAlchemy ORM）
- **认证**: JWT (python-jose)
- **密码加密**: bcrypt (passlib)
- **爬虫**: requests + BeautifulSoup + yt-dlp
- **Markdown转换**: html2text / pandoc（可选）
- **任务调度**: APScheduler

### 前端
- **技术**: 原生 HTML/CSS/JavaScript
- **样式**: 响应式设计，支持PC和移动端
- **API调用**: Fetch API

## 项目结构

```
Learning Archive Platform/
├── backend/
│   ├── app/
│   │   ├── api/              # API路由
│   │   ├── crawler/          # 爬虫模块
│   │   ├── converter/        # 转换模块
│   │   ├── scheduler/        # 任务调度
│   │   ├── storage/          # 存储管理
│   │   ├── utils/            # 工具函数
│   │   ├── config.py         # 配置管理
│   │   ├── database.py       # 数据库模型
│   │   └── main.py           # FastAPI主应用
│   ├── scripts/
│   │   ├── init_db.py        # 数据库初始化
│   │   └── create_user.py    # 创建用户脚本
│   └── requirements.txt      # Python依赖
├── frontend/
│   ├── css/                  # 样式文件
│   ├── js/                   # JavaScript文件
│   └── index.html            # 主页面
└── docs/                     # 文档
```

## 快速开始

### 1. 环境准备

```bash
# 创建虚拟环境
python -m venv venv

# 激活虚拟环境
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# 安装依赖
cd backend
pip install -r requirements.txt
```

### 2. 配置环境变量

```bash
# 复制环境变量示例文件
cp .env.example .env

# 编辑 .env 文件，修改必要的配置（如SECRET_KEY、JWT_SECRET_KEY等）
```

### 3. 初始化数据库

```bash
# 初始化数据库表结构
python scripts/init_db.py
```

### 4. 创建用户

```bash
# 创建管理员用户
python scripts/create_user.py admin your_password
```

### 5. 启动后端服务

```bash
# 方式1: 使用 uvicorn 直接启动
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

# 方式2: 使用 Python 启动
python -m app.main
```

### 6. 访问应用

- **前端页面**: `http://localhost:8000/index.html` (需要配置FastAPI静态文件服务)
- **API文档**: `http://localhost:8000/docs` (FastAPI自动生成的Swagger文档)
- **健康检查**: `http://localhost:8000/health`

## 使用说明

### 1. 登录系统
- 使用创建的用户账号登录
- 登录后获取JWT Token，存储在localStorage中

### 2. 配置采集源
- 在"采集管理"页面添加采集源
- 配置源类型（webpage/video）、URL模式等
- 可以手动触发采集或等待定时任务执行

### 3. 查看文件
- 在"文件列表"页面查看已采集的文件
- 支持按来源、标签筛选
- 支持搜索功能
- 点击文件查看详情

### 4. 下载文件
- 在文件详情页面点击"下载"按钮
- 文件以Markdown格式下载

## 核心功能流程

### 采集流程

1. **定时任务触发**（每天0点，可在配置中修改）
   - 获取所有启用的采集源
   - 遍历每个源执行采集

2. **爬取内容**
   - 根据源类型选择对应的爬虫（网页/视频）
   - 执行爬取，获取内容

3. **内容转换**
   - HTML转Markdown（网页）
   - 字幕文本格式化（视频）
   - 生成目录、标签、摘要

4. **文件保存**
   - 计算内容哈希（去重判断）
   - 如果内容相同，创建新版本
   - 保存文件到文件系统
   - 更新数据库索引

5. **日志记录**
   - 记录采集成功/失败日志
   - 便于问题排查和监控

### 访问流程

1. **用户登录**
   - 验证用户名和密码
   - 生成JWT Token

2. **访问资源**
   - 在请求头中携带Token
   - 后端验证Token有效性

3. **查看文件**
   - 从数据库获取文件列表
   - 读取文件内容并返回

## 配置说明

### 环境变量

- `SECRET_KEY`: 应用密钥（生产环境必须修改）
- `JWT_SECRET_KEY`: JWT签名密钥（生产环境必须修改）
- `DATABASE_URL`: 数据库连接URL
- `COLLECTION_SCHEDULE_HOUR`: 定时任务执行小时（默认0点）
- `COLLECTION_SCHEDULE_MINUTE`: 定时任务执行分钟（默认0分）

### 爬虫配置

- `CRAWLER_USER_AGENT`: 用户代理字符串
- `CRAWLER_REQUEST_DELAY`: 请求延迟（秒）
- `CRAWLER_MAX_RETRIES`: 最大重试次数

## 注意事项

1. **安全性**
   - 生产环境必须修改 `SECRET_KEY` 和 `JWT_SECRET_KEY`
   - 建议配置HTTPS
   - 限制CORS来源

2. **性能**
   - 文件数量较多时，建议添加数据库索引
   - 可以考虑使用PostgreSQL替代SQLite（修改DATABASE_URL）

3. **存储**
   - 定期检查存储空间使用情况
   - 可以配置归档策略清理旧版本

4. **版权**
   - 确保采集的内容仅用于个人学习
   - 遵守相关网站的robots.txt和使用条款

## 后续优化建议

1. **功能增强**
   - 添加全文搜索功能（SQLite FTS）
   - 支持更多内容源（RSS、API等）
   - 文件上传功能完善
   - Markdown渲染优化（使用markdown-it等库）

2. **性能优化**
   - 数据库查询优化
   - 异步采集（多源并行）
   - 缓存策略

3. **用户体验**
   - 前端Markdown渲染优化
   - 添加文件预览功能
   - 版本对比功能
   - 标签管理界面

## 项目状态

✅ **核心功能已完成**：爬虫、转换、存储、调度、API、前端界面

⚠️ **待完善功能**：
- 前端Markdown渲染（目前使用简单文本显示）
- 采集源URL模式匹配（目前简化处理）
- 更多错误处理和用户提示

📝 **建议**：
- 首次使用前先配置采集源并手动触发测试
- 根据实际需求调整采集频率和配置
- 定期备份数据库和文件

---

**项目已完成核心架构和主要功能实现，可以进行测试和部署。**