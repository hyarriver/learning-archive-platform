# 痛点分析与解决方案

## 一、用户痛点

### 1.1 内容零散，难以沉淀

**痛点描述**：
- 学习资料分散在各个网站（博客、文档、视频）
- 无法统一管理和检索
- 难以建立知识体系

**解决方案**：
- ✅ 统一Markdown格式存储
- ✅ 按来源/主题分类组织
- ✅ 支持标签体系，便于关联
- ✅ 文件索引数据库，快速检索

**实现要点**：
- 文件目录结构清晰：`data/collections/{source}/{date}/{file}.md`
- 数据库索引：标题、标签、摘要
- 支持自定义分类和标签

---

### 1.2 网站变动导致资料丢失

**痛点描述**：
- 博客下线、文章删除
- 视频下架、字幕丢失
- 外部依赖不可控

**解决方案**：
- ✅ 本地化存储，完全掌控
- ✅ 版本管理，保留历史
- ✅ 定期自动采集，及时备份
- ✅ 手动补采机制，灵活应对

**实现要点**：
- 文件存储于服务器本地，不依赖外部
- 版本历史保留在 `versions/` 目录
- 采集失败时保留上次成功版本
- 支持手动触发采集

---

### 1.3 视频内容难以快速复习

**痛点描述**：
- 视频需要完整观看，时间成本高
- 难以定位关键知识点
- 字幕检索不便

**解决方案**：
- ✅ 视频字幕转为Markdown文本
- ✅ 自动生成目录（TOC）
- ✅ 支持文本搜索
- ✅ 保留时间戳（可选）

**实现要点**：
- 使用 yt-dlp 提取字幕
- 字幕转Markdown时保留段落结构
- 支持多语言字幕合并
- 可添加时间戳锚点（`[00:12:34]`）

---

### 1.4 资料整理成本高

**痛点描述**：
- 手动复制粘贴耗时
- 格式不统一需要整理
- 需要定期维护更新

**解决方案**：
- ✅ 自动化采集，零人工干预
- ✅ 自动格式转换（统一Markdown）
- ✅ 定时任务，自动更新
- ✅ 智能去重，避免重复存储

**实现要点**：
- 配置采集源后自动执行
- HTML自动转换为标准Markdown
- 文件哈希判断，自动去重
- 采集日志记录，便于追踪

---

## 二、技术痛点

### 2.1 爬虫稳定性问题

**痛点描述**：
- 网站结构变化导致解析失败
- 反爬虫机制限制
- 网络波动导致采集中断

**解决方案**：
- ✅ 容错设计：单URL失败不影响整体
- ✅ 多种解析策略：CSS选择器、XPath、正则
- ✅ 请求头伪装：模拟浏览器
- ✅ 重试机制：指数退避策略
- ✅ 日志记录：失败原因追踪

**实现要点**：
```python
# 伪代码示例
try:
    content = crawler.fetch(url)
    parsed = parser.parse(content)
    if not parsed:
        log.warning(f"解析失败: {url}")
        return None
except Exception as e:
    log.error(f"采集失败: {url}, {e}")
    retry(url, max_retries=3)
```

---

### 2.2 Markdown转换质量

**痛点描述**：
- HTML结构复杂，转换不完整
- 代码块丢失语法信息
- 表格格式错乱

**解决方案**：
- ✅ 使用成熟库：pandoc（高质量转换）
- ✅ 预处理：清理无关HTML元素
- ✅ 后处理：验证代码块、表格
- ✅ 保留原始HTML备份（可选）

**实现要点**：
- 优先使用 pandoc，fallback 到 html2text
- 预处理：去除 `<nav>`, `<footer>`, `<script>`
- 代码块：保留语言标签 ` ```python `
- 表格：转换为Markdown表格语法

---

### 2.3 存储管理复杂度

**痛点描述**：
- 文件数量增长，难以管理
- 版本历史占用空间
- 重复内容浪费存储

**解决方案**：
- ✅ 文件哈希去重
- ✅ 版本差异存储（可选）
- ✅ 定期归档策略
- ✅ 存储使用监控

**实现要点**：
- 文件哈希（MD5/SHA256）判断重复
- 版本仅保留差异（如使用git diff思路）
- 超过阈值时自动归档旧版本
- 数据库记录存储使用情况

---

### 2.4 性能与资源限制

**痛点描述**：
- 40GB存储空间有限
- 200MB带宽限制
- 采集任务耗时

**解决方案**：
- ✅ 增量采集：仅采集新内容
- ✅ 异步并发：多源并行采集
- ✅ 压缩存储：Markdown文本压缩比高
- ✅ 资源监控：及时告警

**实现要点**：
- 基于文件哈希/修改时间判断新内容
- 异步任务：`asyncio` 或 `multiprocessing`
- 定期清理：删除过旧版本或失败记录
- 存储监控：`du -sh` 定期检查

---

## 三、项目落地痛点

### 3.1 开发周期长

**痛点描述**：
- 功能复杂，开发耗时
- 初期难以验证价值

**解决方案**：
- ✅ 最简MVP先行（10-12天）
- ✅ 迭代开发，分阶段交付
- ✅ 先实现单一来源验证

**实施路径**：
1. 第一周：单一博客爬虫 + 基础存储
2. 第二周：定时任务 + 简单前端
3. 第三周：扩展多源 + 优化体验

---

### 3.2 维护成本高

**痛点描述**：
- 爬虫失效需要人工修复
- 内容质量需要人工校验

**解决方案**：
- ✅ 模块化设计，便于替换
- ✅ 配置化采集源，减少代码修改
- ✅ 日志完善，快速定位问题
- ✅ 接受失败，优先可用性

**设计原则**：
- 爬虫与业务逻辑解耦
- 采集源配置JSON化
- 错误隔离，单点不影响全局

---

### 3.3 版权风险

**痛点描述**：
- 爬取内容可能涉及版权
- 存储与传播风险

**解决方案**：
- ✅ 私域使用，不公开传播
- ✅ 不生成公开分享链接
- ✅ 仅登录用户可访问
- ✅ 明确不做内容分发

**实施措施**：
- 强制登录，无匿名访问
- 无分享功能
- 不在搜索引擎索引
- 用户协议明确用途

---

## 四、关键成功因素

### 4.1 必须做到
1. **稳定采集**：每日任务可稳定执行
2. **高质量转换**：Markdown格式正确、完整
3. **长期可用**：数据不丢失、系统不中断
4. **低维护成本**：自动化程度高，人工干预少

### 4.2 避免过度设计
1. **不做搜索推荐**：简单文本搜索足够
2. **不做AI功能**：增加复杂度，无必要
3. **不做商业化**：保持简单专注
4. **不引入复杂中间件**：SQLite足够，无需MySQL/Redis

---

## 五、实施建议

### 5.1 开发顺序
1. **先验证核心价值**：单一来源爬虫 + 转换，验证可行性
2. **再扩展功能**：多源、定时任务、版本管理
3. **最后优化体验**：前端、搜索、管理界面

### 5.2 技术选型原则
- **优先成熟方案**：pandoc、yt-dlp、FastAPI
- **避免过度工程**：SQLite优于MySQL，单机优于分布式
- **保持可替换**：爬虫模块可替换，存储可迁移

### 5.3 质量保证
- **自动化测试**：核心转换逻辑必须有测试
- **日志完善**：采集、错误、访问都有日志
- **人工校验**：初期人工检查样本质量
