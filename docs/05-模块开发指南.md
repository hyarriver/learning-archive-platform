# 模块开发指南

## 一、模块开发规范

### 1.1 代码组织原则

- **单一职责**：每个模块只负责一个功能
- **低耦合**：模块间通过接口通信，减少直接依赖
- **高内聚**：相关功能集中在同一模块
- **可测试**：模块独立，易于单元测试

### 1.2 命名规范

- **模块名**：小写字母 + 下划线（`file_manager.py`）
- **类名**：大驼峰（`FileManager`）
- **函数/变量名**：小写 + 下划线（`get_file_path`）
- **常量**：大写下划线（`MAX_FILE_SIZE`）

---

## 二、核心模块开发指南

### 2.1 爬虫模块 (`backend/app/crawler/`)

#### 基础爬虫类 (`base.py`)

```python
# 伪代码示例
class BaseCrawler:
    """基础爬虫抽象类"""
    
    def __init__(self, config: dict):
        self.config = config
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": settings.crawler_user_agent})
    
    def fetch(self, url: str) -> Optional[str]:
        """获取网页内容（带重试）"""
        # 实现重试逻辑
        pass
    
    def parse(self, content: str) -> Optional[dict]:
        """解析内容（子类实现）"""
        raise NotImplementedError
    
    def crawl(self, url: str) -> Optional[dict]:
        """执行完整爬取流程"""
        content = self.fetch(url)
        if not content:
            return None
        return self.parse(content)
```

#### 网页爬虫 (`webpage.py`)

```python
class WebPageCrawler(BaseCrawler):
    """网页爬虫"""
    
    def parse(self, content: str) -> dict:
        soup = BeautifulSoup(content, 'lxml')
        
        # 提取标题
        title = self._extract_title(soup)
        
        # 提取正文
        body = self._extract_body(soup)
        
        # 提取元数据
        metadata = self._extract_metadata(soup)
        
        return {
            "title": title,
            "content": body,
            "metadata": metadata
        }
    
    def _extract_title(self, soup) -> str:
        # 多种策略：h1 > title > og:title
        pass
    
    def _extract_body(self, soup) -> str:
        # 根据配置的CSS选择器提取正文
        pass
```

#### 视频爬虫 (`video.py`)

```python
class VideoCrawler(BaseCrawler):
    """视频字幕爬虫"""
    
    def parse(self, url: str) -> dict:
        # 使用 yt-dlp 提取字幕
        import yt_dlp
        
        ydl_opts = {
            'writesubtitles': True,
            'writeautomaticsub': True,
            'subtitleslangs': ['zh', 'en'],
            'skip_download': True,
        }
        
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=False)
            subtitles = self._extract_subtitles(info)
        
        return {
            "title": info.get('title'),
            "subtitles": subtitles,
            "duration": info.get('duration')
        }
```

#### 开发要点

- ✅ 错误隔离：单个URL失败不影响整体
- ✅ 重试机制：指数退避，最多3次
- ✅ 请求延迟：避免被封（1-2秒）
- ✅ 日志记录：详细记录失败原因

---

### 2.2 转换模块 (`backend/app/converter/`)

#### Markdown转换 (`to_markdown.py`)

```python
class MarkdownConverter:
    """HTML转Markdown转换器"""
    
    def __init__(self):
        self.use_pandoc = self._check_pandoc()
    
    def convert(self, html: str, title: str = None) -> str:
        """转换HTML为Markdown"""
        if self.use_pandoc:
            return self._convert_with_pandoc(html)
        else:
            return self._convert_with_html2text(html)
    
    def _convert_with_pandoc(self, html: str) -> str:
        # 调用pandoc命令行工具
        import subprocess
        result = subprocess.run(
            ['pandoc', '--from=html', '--to=markdown'],
            input=html.encode('utf-8'),
            capture_output=True,
            check=True
        )
        return result.stdout.decode('utf-8')
    
    def _convert_with_html2text(self, html: str) -> str:
        import html2text
        h = html2text.HTML2Text()
        h.ignore_links = False
        h.ignore_images = False
        return h.handle(html)
```

#### 目录生成 (`toc_generator.py`)

```python
class TOCGenerator:
    """Markdown目录生成器"""
    
    def generate(self, markdown: str) -> str:
        """生成TOC并插入到文档开头"""
        headers = self._extract_headers(markdown)
        toc = self._build_toc(headers)
        return f"{toc}\n\n{markdown}"
    
    def _extract_headers(self, markdown: str) -> List[dict]:
        # 使用正则提取标题
        import re
        pattern = r'^(#{1,6})\s+(.+)$'
        headers = []
        for line in markdown.split('\n'):
            match = re.match(pattern, line)
            if match:
                level = len(match.group(1))
                text = match.group(2)
                headers.append({"level": level, "text": text})
        return headers
    
    def _build_toc(self, headers: List[dict]) -> str:
        toc_lines = ["## 目录\n"]
        for h in headers:
            indent = "  " * (h["level"] - 1)
            link = h["text"].lower().replace(" ", "-")
            toc_lines.append(f"{indent}- [{h['text']}](#{link})")
        return "\n".join(toc_lines)
```

#### 标签提取 (`tag_extractor.py`)

```python
class TagExtractor:
    """标签提取器"""
    
    def extract(self, markdown: str, title: str = None) -> List[str]:
        """提取标签（基于关键词/标题）"""
        tags = []
        
        # 从标题提取
        if title:
            tags.extend(self._extract_from_title(title))
        
        # 从内容提取关键词
        tags.extend(self._extract_keywords(markdown))
        
        return list(set(tags))[:10]  # 最多10个标签
    
    def _extract_from_title(self, title: str) -> List[str]:
        # 简单分词（中文/英文）
        # 可集成 jieba 等分词库
        pass
    
    def _extract_keywords(self, markdown: str) -> List[str]:
        # 提取高频词
        # 过滤停用词
        pass
```

---

### 2.3 存储模块 (`backend/app/storage/`)

#### 文件管理 (`file_manager.py`)

```python
class FileManager:
    """文件管理器"""
    
    def __init__(self):
        self.collections_dir = settings.get_collections_dir()
        self.uploads_dir = settings.get_uploads_dir()
    
    def save_collection(
        self,
        source_name: str,
        title: str,
        content: str,
        date: datetime = None
    ) -> Path:
        """保存采集内容"""
        if date is None:
            date = datetime.now()
        
        # 构建路径：collections/{source}/{date}/{title}.md
        date_str = date.strftime("%Y-%m-%d")
        safe_title = self._sanitize_filename(title)
        file_dir = self.collections_dir / source_name / date_str
        file_dir.mkdir(parents=True, exist_ok=True)
        
        file_path = file_dir / f"{safe_title}.md"
        file_path.write_text(content, encoding='utf-8')
        
        return file_path.relative_to(self.collections_dir)
    
    def _sanitize_filename(self, filename: str) -> str:
        """清理文件名（移除非法字符）"""
        import re
        # 移除非法字符
        filename = re.sub(r'[<>:"/\\|?*]', '', filename)
        # 限制长度
        if len(filename) > 200:
            filename = filename[:200]
        return filename
    
    def calculate_hash(self, content: str) -> str:
        """计算内容哈希（用于去重）"""
        import hashlib
        return hashlib.sha256(content.encode('utf-8')).hexdigest()
```

#### 版本管理 (`version_manager.py`)

```python
class VersionManager:
    """版本管理器"""
    
    def create_version(
        self,
        file_id: int,
        file_path: Path,
        content: str
    ) -> int:
        """创建新版本"""
        # 获取当前最大版本号
        current_max = self._get_max_version(file_id)
        new_version = current_max + 1
        
        # 保存版本文件
        version_dir = file_path.parent / "versions"
        version_dir.mkdir(exist_ok=True)
        version_file = version_dir / f"v{new_version}.md"
        version_file.write_text(content, encoding='utf-8')
        
        # 更新数据库
        # ...
        
        return new_version
    
    def get_version_content(self, file_id: int, version: int) -> str:
        """获取指定版本内容"""
        # 从数据库获取文件路径
        # 读取版本文件
        pass
```

---

### 2.4 API模块 (`backend/app/api/`)

#### 文件列表API (`files.py`)

```python
from fastapi import APIRouter, Depends, Query
from sqlalchemy.orm import Session

router = APIRouter(prefix="/api/files", tags=["files"])

@router.get("/")
async def list_files(
    page: int = Query(1, ge=1),
    page_size: int = Query(20, ge=1, le=100),
    source_id: int = Query(None),
    tag: str = Query(None),
    db: Session = Depends(get_db)
):
    """获取文件列表（分页）"""
    query = db.query(File)
    
    if source_id:
        query = query.filter(File.source_id == source_id)
    
    if tag:
        # 简单JSON搜索（SQLite不支持JSON查询，需自定义）
        query = query.filter(File.tags.contains(tag))
    
    total = query.count()
    items = query.offset((page - 1) * page_size).limit(page_size).all()
    
    return {
        "total": total,
        "page": page,
        "page_size": page_size,
        "items": items
    }

@router.get("/{file_id}")
async def get_file(
    file_id: int,
    db: Session = Depends(get_db)
):
    """获取文件详情"""
    file = db.query(File).filter(File.id == file_id).first()
    if not file:
        raise HTTPException(404, "文件不存在")
    
    # 读取文件内容
    full_path = settings.get_collections_dir() / file.file_path
    content = full_path.read_text(encoding='utf-8')
    
    return {
        "id": file.id,
        "title": file.title,
        "content": content,
        "tags": json.loads(file.tags) if file.tags else [],
        "created_at": file.created_at
    }
```

---

## 三、开发最佳实践

### 3.1 错误处理

```python
# 统一异常类
class CrawlerError(Exception):
    """爬虫异常"""
    pass

class ConversionError(Exception):
    """转换异常"""
    pass

# 使用示例
try:
    result = crawler.crawl(url)
except CrawlerError as e:
    logger.error(f"爬虫失败: {url}, {e}")
    return None
except Exception as e:
    logger.exception(f"未知错误: {url}")
    raise
```

### 3.2 日志记录

```python
import logging

logger = logging.getLogger(__name__)

# 记录关键操作
logger.info(f"开始采集: {url}")
logger.warning(f"解析失败，使用备用策略: {url}")
logger.error(f"采集失败: {url}, {error}")
```

### 3.3 配置管理

```python
# 使用Pydantic Settings
from app.config import settings

# 访问配置
delay = settings.crawler_request_delay
max_retries = settings.crawler_max_retries
```

### 3.4 测试示例

```python
# tests/test_crawler.py
import pytest
from app.crawler.webpage import WebPageCrawler

def test_webpage_crawler():
    crawler = WebPageCrawler(config={})
    result = crawler.crawl("https://example.com")
    assert result is not None
    assert "title" in result
    assert "content" in result
```

---

## 四、模块依赖关系

```
main.py
  ├── api/
  │   ├── auth.py ──→ utils/auth.py
  │   ├── files.py ──→ storage/file_manager.py
  │   └── collection.py ──→ crawler/, scheduler/
  │
  ├── crawler/
  │   ├── base.py ──→ config.py
  │   ├── webpage.py ──→ base.py
  │   └── video.py ──→ base.py
  │
  ├── converter/
  │   ├── to_markdown.py
  │   ├── toc_generator.py
  │   └── tag_extractor.py
  │
  ├── storage/
  │   ├── file_manager.py ──→ config.py
  │   └── version_manager.py ──→ database.py
  │
  └── scheduler/
      └── tasks.py ──→ crawler/, converter/, storage/
```

---

## 五、开发顺序建议

1. **先实现核心流程**：爬虫 → 转换 → 存储
2. **再完善功能**：版本管理、标签提取
3. **最后优化体验**：API、前端、错误处理

遵循"先跑通，再优化"的原则。
